{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c97461e8-61b5-4514-9d94-1b493d0cff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMSECCENS    425\n",
      "NUMERO        425\n",
      "kw_pct        425\n",
      "he_pct        425\n",
      "dtype: int64\n",
      "NUMSECCENS    135\n",
      "NUMERO        135\n",
      "kw_pct        135\n",
      "he_pct        135\n",
      "dtype: int64\n",
      "NUMSECCENS          425\n",
      "NUMERO              425\n",
      "kw_pct              425\n",
      "he_pct              425\n",
      "NOMBRE              425\n",
      "median_price_inf    425\n",
      "dtype: int64\n",
      "NUMSECCENS          135\n",
      "NUMERO              135\n",
      "kw_pct              135\n",
      "he_pct              135\n",
      "NOMBRE              135\n",
      "median_price_inf    135\n",
      "dtype: int64\n",
      "   NUMSECCENS  NUMERO    kw_pct    he_pct  NOMBRE  median_price_inf\n",
      "0      1001.0     1.0  0.410302  0.374126  CENTRO       2361.290759\n",
      "1      1002.0     1.0  0.899441  0.832168  CENTRO       2361.290759\n",
      "2      1003.0     1.0  0.924686  0.903226  CENTRO       2361.290759\n",
      "3      1004.0     1.0  0.305556  0.357447  CENTRO       2361.290759\n",
      "4      2001.0     2.0  0.333333  0.812500    ESTE       2846.996138\n",
      "   NUMSECCENS  NUMERO    kw_pct    he_pct  NOMBRE  median_price_inf\n",
      "0      1001.0     1.0  0.576744  0.413043  CENTRO       3097.220365\n",
      "1      2003.0     2.0  0.454545  0.282443    ESTE       3240.393552\n",
      "2      2004.0     2.0  0.752809  0.625000    ESTE       3240.393552\n",
      "3      2009.0     2.0  0.359223  0.268868    ESTE       3240.393552\n",
      "4      2013.0     2.0  0.698925  0.543253    ESTE       3240.393552\n",
      "Any infinite values? False\n",
      "Any NaN values? False\n",
      "Data scaled and transformed.\n",
      "The amount of explained variance of the SES score using each component is...\n",
      "[0.62254378 0.29531581 0.08214041]\n",
      "The amount of explained variance of the SES score is: 0.62254\n",
      "            he_pct_11  kw_pct_11  median_price_inf_11    SES_11  he_pct_21  \\\n",
      "NUMSECCENS                                                                   \n",
      "1001.0       0.374126   0.410302          2361.290759  1.165939   0.374126   \n",
      "1002.0       0.832168   0.899441          2361.290759  2.336438   0.832168   \n",
      "1003.0       0.903226   0.924686          2361.290759  2.475125   0.903226   \n",
      "1004.0       0.357447   0.305556          2361.290759  1.049670   0.357447   \n",
      "2001.0       0.812500   0.333333          2846.996138  3.112318   0.812500   \n",
      "2002.0       0.600000   0.639831          2846.996138  3.021175   0.600000   \n",
      "2003.0       0.287688   0.406828          2846.996138  2.308229   0.287688   \n",
      "2004.0       0.662239   0.725379          2846.996138  3.196389   0.662239   \n",
      "2005.0       0.890909   0.930502          2846.996138  3.747643   0.890909   \n",
      "\n",
      "            kw_pct_21  median_price_inf_21    SES_21  \n",
      "NUMSECCENS                                            \n",
      "1001.0       0.410302          2361.290759  1.165939  \n",
      "1002.0       0.899441          2361.290759  2.336438  \n",
      "1003.0       0.924686          2361.290759  2.475125  \n",
      "1004.0       0.305556          2361.290759  1.049670  \n",
      "2001.0       0.333333          2846.996138  3.112318  \n",
      "2002.0       0.639831          2846.996138  3.021175  \n",
      "2003.0       0.406828          2846.996138  2.308229  \n",
      "2004.0       0.725379          2846.996138  3.196389  \n",
      "2005.0       0.930502          2846.996138  3.747643  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import decomposition  \n",
    "from sklearn.preprocessing import scale  \n",
    "from sklearn import preprocessing \n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "### Importar Datos\n",
    "\n",
    "## 2011\n",
    "\n",
    "df_vivienda_2011 = pd.read_csv(r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Variables\\01  - Precio Vivienda\\Distritos\\2011_Distritos_PrecioVivienda_V2.csv\", sep=\";\")\n",
    "df_ocupacion_2011 = pd.read_csv(r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Variables\\03 - Ocupacion\\Secciones\\2011_Secciones_Ocupacion_V2.csv\", sep=\";\")\n",
    "df_estudios_2011 = pd.read_csv(r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Variables\\04  - Estudios\\Secciones\\2011_Secciones_Estudios_Detalle_V2.csv\", sep=\";\")\n",
    "\n",
    "## 2016\n",
    "\n",
    "df_vivienda_2021 = pd.read_csv(r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Variables\\01  - Precio Vivienda\\Distritos\\2021_Distritos_PrecioVivienda_V2.csv\", sep=\";\")\n",
    "df_ocupacion_2021 = pd.read_csv(r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Variables\\03 - Ocupacion\\Secciones\\2021_Secciones_Ocupacion_V2.csv\", sep=\";\")\n",
    "df_estudios_2021 = pd.read_csv(r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Variables\\04  - Estudios\\Secciones\\2021_Secciones_Estudios_Detalle_V2.csv\", sep=\";\")\n",
    "\n",
    "path_scores = r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Scores\"\n",
    "path_analytical = r\"C:\\Users\\Usuario\\OneDrive\\Escritorio\\UOC\\TFM\\PEC3  - Implementacion\\WorkStation\\Datos\\Analytical\"\n",
    "\n",
    "#Para poder unir los valores de vivienda que solo estan por distrito con las secciones que son los datos de ocupacion y estudios se crea la columna NUMERO en estos dos\n",
    "df_ocupacion_2011['NUMERO'] = df_ocupacion_2011['NUMSECCENS'].astype(str).str[:-5].astype(float)\n",
    "df_ocupacion_2021['NUMERO'] = df_ocupacion_2021['NUMSECCENS'].astype(str).str[:-5].astype(float)\n",
    "df_estudios_2011['NUMERO'] = df_estudios_2011['NUMSECCENS'].astype(str).str[:-5].astype(float)\n",
    "df_estudios_2021['NUMERO'] = df_estudios_2021['NUMSECCENS'].astype(str).str[:-5].astype(float)\n",
    "\n",
    "#Eliminar Martiricos-La Roca porque no hay datos de 2011 en vivienda\n",
    "df_estudios_2011 = df_estudios_2011[df_estudios_2011['NUMERO'] != 5]\n",
    "df_estudios_2021 = df_estudios_2021[df_estudios_2021['NUMERO'] != 5]\n",
    "df_ocupacion_2011 = df_ocupacion_2011[df_ocupacion_2011['NUMERO'] != 5]\n",
    "df_ocupacion_2021 = df_ocupacion_2021[df_ocupacion_2021['NUMERO'] != 5]\n",
    "df_vivienda_2011 = df_vivienda_2011[df_vivienda_2011['NUMERO'] != 5]\n",
    "df_vivienda_2021 = df_vivienda_2021[df_vivienda_2021['NUMERO'] != 5]\n",
    "\n",
    "#Unir estudios y ocupacion por la seccion\n",
    "df_estudios_ocupacion_2011 = pd.merge(df_estudios_2011, df_ocupacion_2011, on='NUMSECCENS', how='inner')[[\"NUMSECCENS\",\"NUMERO_x\",\"kw_pct\",\"he_pct\"]]\n",
    "df_estudios_ocupacion_2021 = pd.merge(df_estudios_2021, df_ocupacion_2021, on='NUMSECCENS', how='inner')[[\"NUMSECCENS\",\"NUMERO_x\",\"kw_pct\",\"he_pct\"]]\n",
    "df_estudios_ocupacion_2011.rename(columns={'NUMERO_x': 'NUMERO'}, inplace=True)\n",
    "df_estudios_ocupacion_2021.rename(columns={'NUMERO_x': 'NUMERO'}, inplace=True)\n",
    "print(df_estudios_ocupacion_2011.count())\n",
    "print(df_estudios_ocupacion_2021.count())\n",
    "\n",
    "#Unir vivienda y (estudios-ocupacion) por distrito\n",
    "df_vivienda_estudios_ocupacion_2011 = pd.merge(df_estudios_ocupacion_2011, df_vivienda_2011, on='NUMERO', how='left')\n",
    "df_vivienda_estudios_ocupacion_2021 = pd.merge(df_estudios_ocupacion_2021, df_vivienda_2021, on='NUMERO', how='left')\n",
    "print(df_vivienda_estudios_ocupacion_2011.count())\n",
    "print(df_vivienda_estudios_ocupacion_2021.count())\n",
    "print(df_vivienda_estudios_ocupacion_2011.head())\n",
    "print(df_vivienda_estudios_ocupacion_2021.head())\n",
    "\n",
    "df11 = df_vivienda_estudios_ocupacion_2011.set_index('NUMSECCENS')[[\"he_pct\",\"kw_pct\",\"median_price_inf\"]]\n",
    "df21 = df_vivienda_estudios_ocupacion_2011.set_index('NUMSECCENS')[[\"he_pct\",\"kw_pct\",\"median_price_inf\"]]\n",
    "\n",
    "checks = {\n",
    "    \"Qualifications 2011\":df11[\"he_pct\"],\n",
    "    \"Qualifications 2021\":df21[\"he_pct\"],\n",
    "    \"Occupations 2011\":df11[\"kw_pct\"],\n",
    "    \"Occupations 2021\":df21[\"kw_pct\"],\n",
    "    \"House Prices 2011\":df11[\"median_price_inf\"],\n",
    "    \"House Prices 2021\":df21[\"median_price_inf\"],\n",
    "}\n",
    "\n",
    "for k, v in checks.items():\n",
    "    if (np.isnan(v.values).any()):\n",
    "        print(\"Have null values in data set: \" + k)\n",
    "\n",
    "#  Create dataset of indicator data - 2001\n",
    "res_11 = pd.concat([df11[\"he_pct\"],df11[\"kw_pct\"],df11[\"median_price_inf\"]], axis=1)\n",
    "res_21 = pd.concat([df21[\"he_pct\"],df21[\"kw_pct\"],df21[\"median_price_inf\"]], axis=1)\n",
    "\n",
    "# Create dataset of indicator data\n",
    "X_11 = res_11.values\n",
    "X_21 = res_21.values\n",
    "\n",
    "#  Join 2001 and 2011 datasets and sanity-check\n",
    "SES_inds = np.concatenate((X_11, X_21), axis=0)\n",
    "\n",
    "#print(SES_inds)\n",
    "\n",
    "print(\"Any infinite values? \" + str(~np.isfinite(SES_inds).any()))\n",
    "print(\"Any NaN values? \" + str(np.isnan(SES_inds).any()))\n",
    "\n",
    "#  Median removal and Unit scaling\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(SES_inds)\n",
    "SES_inds = scaler.transform(SES_inds)\n",
    "\n",
    "print(\"Data scaled and transformed.\")\n",
    "\n",
    "pca_full = decomposition.PCA()                           # Use all Principal Components\n",
    "pca_full.fit(SES_inds)                                   # Train model on data\n",
    "SES_full_T = pd.DataFrame(pca_full.transform(SES_inds))  # Transform data using model\n",
    "\n",
    "print(\"The amount of explained variance of the SES score using each component is...\")\n",
    "print(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Adapted from https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn\n",
    "i = np.identity(SES_inds.shape[1])  # identity matrix\n",
    "\n",
    "coef = pca_full.transform(i)\n",
    "\n",
    "loadings = pd.DataFrame(coef, index=res_11.columns)\n",
    "loadings.to_csv(os.path.join(path_scores,\"NoTransform\" + '-Loadings-Secciones-2021.csv.gz'), compression='gzip', index=True, sep=\";\")\n",
    "\n",
    "#  Fitting PCA Model to derive SES score\n",
    "pca = decomposition.PCA(n_components=1)             # Only need 1st Principal Component\n",
    "pca.fit(SES_inds)                                   #  Train model on data\n",
    "SES_inds_T = pd.DataFrame(pca.transform(SES_inds))  #  Transform data using model\n",
    "\n",
    "print(\"The amount of explained variance of the SES score is: {0:6.5f}\".format(pca.explained_variance_ratio_[0]))\n",
    "\n",
    "#  Split transformed data into 2011 and 2021 datasets\n",
    "#  Note the way we do this to deal with missing data (if any)\n",
    "scores_11 = SES_inds_T.loc[0:len(X_11)-1,0]\n",
    "scores_21 = SES_inds_T.loc[len(X_21):,0]\n",
    "\n",
    "# Create dfs from the two sets of scores\n",
    "res_11 = res_11.assign(scores=pd.Series(scores_11).values)\n",
    "res_21 = res_21.assign(scores=pd.Series(scores_21).values)\n",
    "\n",
    "#res.columns = ['LSOANM','PRICE-01','QUALS-01','OCC-01','INCOME-01','PRICE-11',\n",
    "#               'QUALS-11','OCC-11','INCOME-11','SES_01','SES_11']\n",
    "\n",
    "# Join them together so we've got a single df for 2011 and 2021\n",
    "res = res_11.merge(res_21, how='outer', suffixes=('_11','_21'), left_index=True, right_index=True)\n",
    "\n",
    "# Rename columns for consistency with Jordan's code\n",
    "res.rename(columns={'scores_11':'SES_11', 'scores_21':'SES_21'}, inplace=True)\n",
    "\n",
    "# Sanity check\n",
    "print(res.head(9))\n",
    "\n",
    "#  Compute rank of LSOA in 2011 (so low rank = 'low status')\n",
    "res['RANK_11'] = res.SES_11.rank(ascending=False)\n",
    "\n",
    "#  Compute rank of LSOA in 2021 (so low rank = 'low status')\n",
    "res['RANK_21'] = res.SES_21.rank(ascending=False)\n",
    "\n",
    "#  Compute amount by which LSOA has ascended (so +ve = status improvement; -ve = status decline)\n",
    "res.loc[:,'SES_ASC'] = res.loc[:,'SES_21'] - res.loc[:,'SES_11']\n",
    "\n",
    "import re \n",
    "#  Calculate LSOA percentile score in 01\n",
    "res.loc[:,'SES_PR_11'] = res.RANK_11.rank(ascending=False, pct=True) * 100\n",
    "\n",
    "#  Calculate LSOA percentile score in 11\n",
    "res.loc[:,'SES_PR_21'] = res.RANK_21.rank(ascending=False, pct=True) * 100\n",
    "\n",
    "#  Calculate percentile change (so +ve = 'moved up' in the world; -ve = 'moved down')\n",
    "res.loc[:,'SES_PR_ASC'] = res.loc[:,'SES_PR_21'] - res.loc[:,'SES_PR_11']\n",
    "\n",
    "inp = res.loc[:,[x for x in res.columns if 'SES' not in x and 'RANK' not in x]]\n",
    "\n",
    "# Tidy up the naming\n",
    "inp.rename(columns=lambda x: re.sub('_21',' 2021',re.sub('_11',' 2011',x)), inplace=True)\n",
    "inp.rename(columns=lambda x: re.sub('kw_pct','Knowledge Worker Percentage',x), inplace=True)\n",
    "inp.rename(columns=lambda x: re.sub('he_pct','Highly-Educated Percentage',x), inplace=True)\n",
    "inp.rename(columns=lambda x: re.sub('median_price_inf','Property Prices (Transformed)',x), inplace=True)\n",
    "\n",
    "# Save to file (note that we are also saving some info about the input variables as we use these as well)\n",
    "res[\n",
    "    ['RANK_11','RANK_21','SES_11','SES_21','SES_ASC','SES_PR_11','SES_PR_21','SES_PR_ASC']\n",
    "].to_csv(os.path.join(path_analytical,\"No transform\" + '-Scores-Secciones.csv.gz'), compression='gzip', index=True, sep=\";\") \n",
    "inp[\n",
    "    [x for x in inp.columns if '2011' in x]\n",
    "].to_csv(os.path.join(path_scores,\"No transform\" + '-Inputs-Secciones-2011.csv.gz'), compression='gzip', index=True, sep=\";\")\n",
    "inp[\n",
    "    [x for x in inp.columns if '2021' in x]\n",
    "].to_csv(os.path.join(path_scores,\"No transform\" + '-Inputs-Secciones-2021.csv.gz'), compression='gzip', index=True, sep=\";\")\n",
    "\n",
    "res.to_csv(os.path.join(path_scores,\"Scores-Secciones.csv\"),index=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f2df0-d566-4c71-a145-597a0df428bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789768c-f322-4cdb-9037-8cb1c347acda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
